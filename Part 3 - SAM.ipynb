{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVcEtGJ8QeYP"
   },
   "source": [
    "# 3. Sharpness-aware minimization (SAM) wirh Sparse Networks - 25 points\n",
    "\n",
    "## 3.1 Get a sparse networks through pruning\n",
    "**Pruning** is a technique used to reduce the size and complexity of a neural network model by removing (setting to zero) less important parameters. The goal is to create a more efficient model that retains its predictive accuracy while being smaller, which can improve both inference speed and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilWbesreILh4"
   },
   "source": [
    "Let's train a simple model on the MNIST dataset to learn about pruning at first. We just use 10% of the dataset for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJta4OIBcEpy"
   },
   "source": [
    "### 3.1.1 Train a dense network with SGD\n",
    "Let us first train a dense model with SGD. We reuse the model for the discriminator of the GAN in Homework 2 and name it 'Classifier'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXSZdsCILh4"
   },
   "outputs": [],
   "source": [
    "from lib.part3.utils import *\n",
    "max_epochs = 10\n",
    "device = \"cpu\" # Change this if you can and want to use a GPU device\n",
    "model = Classifier().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLI-Alb-ILh5"
   },
   "source": [
    "We define the optimizing process of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oXZG0iPPILh5"
   },
   "outputs": [],
   "source": [
    "def optimize_sgd(model, optimizer, img, label):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(img)\n",
    "    loss = cross_entropy(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udzDwcQqX-GE"
   },
   "source": [
    "The following cell runs the training loop, this might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y_p6FbG0ILh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 8.66MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 277kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.76MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.87MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Epoch 0 with 0.932 accuracy on the validation set.\n",
      "Epoch 1 with 0.948 accuracy on the validation set.\n",
      "Epoch 2 with 0.956 accuracy on the validation set.\n",
      "Epoch 3 with 0.958 accuracy on the validation set.\n",
      "Epoch 4 with 0.964 accuracy on the validation set.\n",
      "Epoch 5 with 0.963 accuracy on the validation set.\n",
      "Epoch 6 with 0.965 accuracy on the validation set.\n",
      "Epoch 7 with 0.966 accuracy on the validation set.\n",
      "Epoch 8 with 0.968 accuracy on the validation set.\n",
      "Epoch 9 with 0.967 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, optimize_sgd, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnvFT-DxILh6"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LKTkbxGyILh6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9768 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgVCA_jwJbPx"
   },
   "source": [
    "### 3.1.2 Sparse network with magnitude-based pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubynCgs2b0wT"
   },
   "source": [
    "Magnitude-based pruning specifically focuses on **removing weights that have the smallest absolute values**, under the assumption that weights with smaller magnitudes contribute less to the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkjQPvAXcZcI"
   },
   "source": [
    "**(1)** (6 points) Realize magnitude-based pruning below, which removes a part of weights that have the smallest absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfb19GShJZFI"
   },
   "outputs": [],
   "source": [
    "def magnitude_prune(model, prune_fraction):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            # FILL: Get weight's absolute values\n",
    "            abs_weight = param.data.abs()\n",
    "            # FILL: Compute the threshold\n",
    "            threshold = torch.quantile(abs_weight.view(-1), prune_fraction)\n",
    "            # FILL: Prune weights below the threshold\n",
    "            mask = abs_weight >= threshold\n",
    "            param.data.mul_(mask)   # zero out pruned weights\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fr0VzGC_NPtv"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# Copy a model for pruning\n",
    "sparse_model = copy.deepcopy(model)\n",
    "# Get a sparse model by pruning 50% parameters\n",
    "sparse_model = magnitude_prune(sparse_model, prune_fraction=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sOOCdBoRgzL"
   },
   "source": [
    "Copy a sparse model for SAM implementation later in 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Xzi_XqlmRbC4"
   },
   "outputs": [],
   "source": [
    "sparse_model_sam = copy.deepcopy(sparse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_EFs4FiQh1X"
   },
   "source": [
    "Evaluation after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4ZtXqwuaPtNF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9422 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz5gPbMuGiAW"
   },
   "source": [
    "### 3.1.3 Finetune the sparse model\n",
    "\n",
    "We finetune the sparse model after pruning with SGD to recover its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jD0tZKllNi00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.93 accuracy on the validation set.\n",
      "Epoch 1 with 0.93 accuracy on the validation set.\n",
      "Epoch 2 with 0.93 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "finetune_epoch = 3\n",
    "train_model(sparse_model, optimizer, optimize_sgd, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rha2SQXBQnV8"
   },
   "source": [
    "Evaluate the sparse model after finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sby9YSR9PrYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9468 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHZY8_nZaMO-"
   },
   "source": [
    "**(2)** (2 point) What are the pros and cons of sparse networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse networks reduce the number of nonzero parameters, which can lower memory usage and (when the sparsity pattern is actually exploited by the hardware/software stack) reduce computation and speed up inference. Sparsity can also act as a form of regularization, sometimes improving generalization by removing weak weights.  \n",
    "\n",
    "In practice, unstructured sparsity often yields little or no GPU speedup because dense kernels are highly optimized, while sparse kernels can suffer from overhead and irregular memory access. Accuracy may degrade after pruning, especially at high sparsity levels, so fine-tuning is usually required to recover performance. It also adds engineering complexity, since masking, sparse storage formats, and sparsity-aware tooling complicate both training and inference pipelines. Finally, the gains are hardware-dependent, since meaningful acceleration typically requires structured sparsity or specialized libraries and accelerators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97IxkV9Rl5bT"
   },
   "source": [
    "## 3.2 Train the sparse model with SAM\n",
    "\n",
    "Sharpness-aware minimization (SAM) is a new optimization technique, which is satisfied with not just a low loss, instead it seeks a neighborhood with uniformly low loss. SAM is motivated by the link between the geometry of the loss landscape and generalization. It makes sense that a low loss within a uniformly low loss neighborhood will generalize better than a low loss within a region of higher variance.\n",
    "\n",
    "To be specific, we consider a model with the weight vector of $\\mathbf{w}$ and the training loss $L_S$. SAM aims to minimize the maximum loss within a small region which is usually a $\\ell_2$ ball with $\\rho$ radius. Note that $\\rho$ is a small value close to $0$. Therefore, SAM can be formulated as a minimax optimization problem:\n",
    "$$\\min_{\\mathbf{w}} \\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2\\leq \\rho} L_S (\\mathbf{w} + \\mathbf{\\epsilon})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFY_FOCEILh3"
   },
   "source": [
    "**(3)** (3 points) Please solve the inner maximum problem by first-order Taylor expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u8vP9FCtTuh"
   },
   "source": [
    "Using a first-order Taylor expansion around $\\mathbf{w}$,\n",
    "$$\n",
    "L_S(\\mathbf{w}+\\boldsymbol{\\epsilon})\n",
    "\\approx\n",
    "L_S(\\mathbf{w}) + \\nabla_{\\mathbf{w}}L_S(\\mathbf{w})^\\top \\boldsymbol{\\epsilon}.\n",
    "$$\n",
    "so the inner problem reduces to maximizing the linear form $\\nabla_{\\mathbf{w}}L_S(\\mathbf{w})^\\top \\boldsymbol{\\epsilon}$ over the $\\ell_2$ ball $\\{\\boldsymbol{\\epsilon}:\\|\\boldsymbol{\\epsilon}\\|_2\\le \\rho\\}$.\n",
    "\n",
    "Hence the inner maximization becomes\n",
    "$$\n",
    "\\max_{\\|\\boldsymbol{\\epsilon}\\|_2\\le \\rho} L_S(\\mathbf{w}+\\boldsymbol{\\epsilon})\n",
    "\\approx\n",
    "L_S(\\mathbf{w}) + \\max_{\\|\\boldsymbol{\\epsilon}\\|_2\\le \\rho}\\nabla_{\\mathbf{w}}L_S(\\mathbf{w})^\\top \\boldsymbol{\\epsilon}.\n",
    "$$\n",
    "By Cauchy–Schwarz,\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}L_S(\\mathbf{w})^\\top \\boldsymbol{\\epsilon}\n",
    "\\le \\|\\nabla_{\\mathbf{w}}L_S(\\mathbf{w})\\|_2\\,\\|\\boldsymbol{\\epsilon}\\|_2\n",
    "\\le \\rho \\|\\nabla_{\\mathbf{w}}L_S(\\mathbf{w})\\|_2,\n",
    "$$\n",
    "and the maximum is achieved when $\\boldsymbol{\\epsilon}$ aligns with the gradient:\n",
    "$$\n",
    "\\boldsymbol{\\epsilon}^\\star\n",
    "=\n",
    "\\rho \\frac{\\nabla_{\\mathbf{w}}L_S(\\mathbf{w})}{\\|\\nabla_{\\mathbf{w}}L_S(\\mathbf{w})\\|_2}.\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\max_{\\|\\boldsymbol{\\epsilon}\\|_2\\le \\rho} L_S(\\mathbf{w} + \\boldsymbol{\\epsilon})\n",
    "\\approx\n",
    "L_S(\\mathbf{w}) + \\rho \\|\\nabla_{\\mathbf{w}}L_S(\\mathbf{w})\\|_2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwbpa28BILh6"
   },
   "source": [
    "**(4)** (8 points) Now we will train the same model using the SAM optimizer.\n",
    "Please implement SAM by the two steps below. The first step is for the maximizer which calculates $\\epsilon$ obtained in question (1). The second step is the normal step for the minimizer: $\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta_t \\nabla L_S (\\mathbf{w}_t + \\mathbf{\\epsilon}_t)$ where $\\eta_t$ is step size. Note that we set $\\rho=0.05$.\n",
    "\n",
    "Hint: be careful about weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mZeWh05ILh6"
   },
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, lr=0.01, rho=0.05):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, lr)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        # Note that p.grad gets the gradient; p.data gets the weight.\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        norm += 1e-12 # Avoid zero norm\n",
    "        return norm\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self):\n",
    "        # Add the perturbation on the weight.\n",
    "        grad_norm = self._grad_norm()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            rho = group[\"rho\"]\n",
    "            scale = rho / grad_norm\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                # save current weights\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "\n",
    "                # ascent step: w <- w + epsilon\n",
    "                e_w = p.grad * scale\n",
    "                p.add_(e_w)\n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.data.copy_(self.state[p][\"old_p\"])\n",
    "\n",
    "        self.base_optimizer.step()\n",
    "\n",
    "        self.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F6EaFSWILh6"
   },
   "source": [
    "Define an optimizer of `SAM` for the model. We recommend using `SGD` as base optimizer with a learning rate of $0.05$ (which is same with SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f3D9IZGql74k"
   },
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.SGD\n",
    "sam_optimizer = SAM(sparse_model_sam.parameters(), base_optimizer, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwYNhqGoILh6"
   },
   "source": [
    "**(5)** (4 points) Please define the optimizing process of SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9-zRaSVCwGO"
   },
   "outputs": [],
   "source": [
    "def optimize_sam(model, optimizer, img, label):\n",
    "\n",
    "    enable_running_stats(model)\n",
    "    # First forward-backward pass\n",
    "    output = model(img)\n",
    "    loss = cross_entropy(output, label)\n",
    "    loss.backward()              # fills p.grad at w\n",
    "    optimizer.first_step()       # w <- w + eps, and zero_grad()\n",
    "\n",
    "\n",
    "    disable_running_stats(model)\n",
    "    # Second forward-backward pass\n",
    "    output_perturbed = model(img)\n",
    "    loss_perturbed = cross_entropy(output_perturbed, label)\n",
    "    loss_perturbed.backward()    # fills p.grad at w+eps\n",
    "    optimizer.second_step()      # restore w, then base_optimizer.step(), then zero_grad()\n",
    "\n",
    "    return loss_perturbed.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "45KsL4m4DAU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.963 accuracy on the validation set.\n",
      "Epoch 1 with 0.965 accuracy on the validation set.\n",
      "Epoch 2 with 0.966 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(sparse_model_sam, sam_optimizer, optimize_sam, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhSh_GOxoNE-"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the sparse model finetuned with SAM on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3IAfT8aMoNWb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9778 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model_sam)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi2NLImh_QeD"
   },
   "source": [
    "**(6)** (2 points) Give a conclusion comparing SAM with SGD. Is there any drawback of SAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our MNIST pruning experiment, the dense model trained with SGD reached a test accuracy of $0.9768$. After $50%$ magnitude-based pruning, accuracy dropped to $0.9422$. Finetuning the sparse model with SGD for $3$ epochs only slightly improved performance to $0.9468$ (recovering $0.0046$), whereas finetuning with SAM for $3$ epochs achieved $0.9778$ (recovering $0.0356$), which is $0.0310$ higher than the SGD-finetuned sparse model and even $0.0010$ higher than the original dense baseline.\n",
    "\n",
    "After pruning, the network has less redundancy and becomes more sensitive: small changes to the remaining weights can cause larger changes in the output, so SGD can easily settle in a sharp minimum that fits the training data but is not robust, which helps explain the limited recovery from $0.9422$ to $0.9468$. In contrast, SAM updates parameters using gradients evaluated at a worst-case perturbed point $w+\\epsilon$ within an $\\ell_2$ ball of radius $\\rho$, encouraging solutions where the loss stays low not only at $w$ but throughout a neighborhood around it. This biases training toward flatter, more robust regions of the loss landscape, which typically generalize better; in our experiment this robustness appears crucial after pruning, since SAM recovers accuracy from $0.9422$ to $0.9778$ and slightly surpasses the dense baseline ($0.9768$).\n",
    "\n",
    "The main drawbacks of SAM are practical. First, it is substantially more expensive than SGD because each update requires two forward-backward passes (one to compute the ascent direction $\\epsilon$ and one to compute the gradient at $w+\\epsilon$), which roughly doubles training time and increases memory usage. Second, SAM introduces extra hyperparameter sensitivity: the neighborhood size $\\rho$ interacts with the learning rate, weight decay, and momentum, so achieving good performance often requires retuning and can slow convergence if $\\rho$ is too large (over-regularization) or too small (little benefit). Third, the implementation is more complex, beacuse parameters must be perturbed and then exactly restored before applying the base optimizer step, gradients must be carefully zeroed between the two passes, and BatchNorm running statistics typically need special handling so they are not updated on the perturbed forward pass."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
